digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140348614687296 [label="
 ()" fillcolor=darkolivegreen1]
	140348618198080 [label=SmoothL1LossBackward0]
	140348618198608 -> 140348618198080
	140348618198608 [label=IndexBackward0]
	140348618174128 -> 140348618198608
	140348618174128 [label=SqueezeBackward0]
	140348618173984 -> 140348618174128
	140348618173984 [label=ReluBackward0]
	140348618173648 -> 140348618173984
	140348618173648 [label=AddmmBackward0]
	140348618171152 -> 140348618173648
	140348618118864 [label="neighGen.dGen.reg.bias
 (1)" fillcolor=lightblue]
	140348618118864 -> 140348618171152
	140348618171152 [label=AccumulateGrad]
	140348618173504 -> 140348618173648
	140348618173504 [label=AddBackward0]
	140348618173024 -> 140348618173504
	140348618173024 [label=AddmmBackward0]
	140348618171200 -> 140348618173024
	140348618119824 [label="neighGen.encoder.layers.1.lin_l.bias
 (128)" fillcolor=lightblue]
	140348618119824 -> 140348618171200
	140348618171200 [label=AccumulateGrad]
	140348618172592 -> 140348618173024
	140348618172592 [label=DivBackward0]
	140348618171536 -> 140348618172592
	140348618171536 [label=ScatterAddBackward0]
	140348618173360 -> 140348618171536
	140348618173360 [label=IndexSelectBackward0]
	140348618173840 -> 140348618173360
	140348618173840 [label=NativeDropoutBackward0]
	140348618172064 -> 140348618173840
	140348618172064 [label=ReluBackward0]
	140348618171728 -> 140348618172064
	140348618171728 [label=AddBackward0]
	140348618170576 -> 140348618171728
	140348618170576 [label=AddmmBackward0]
	140348618170912 -> 140348618170576
	140348618078704 [label="neighGen.encoder.layers.0.lin_l.bias
 (64)" fillcolor=lightblue]
	140348618078704 -> 140348618170912
	140348618170912 [label=AccumulateGrad]
	140348618173408 -> 140348618170576
	140348618173408 [label=TBackward0]
	140348618170768 -> 140348618173408
	140348618079744 [label="neighGen.encoder.layers.0.lin_l.weight
 (64, 1433)" fillcolor=lightblue]
	140348618079744 -> 140348618170768
	140348618170768 [label=AccumulateGrad]
	140348618173552 -> 140348618171728
	140348618173552 [label=MmBackward0]
	140348618170528 -> 140348618173552
	140348618170528 [label=TBackward0]
	140348618197696 -> 140348618170528
	140348618145936 [label="neighGen.encoder.layers.0.lin_r.weight
 (64, 1433)" fillcolor=lightblue]
	140348618145936 -> 140348618197696
	140348618197696 [label=AccumulateGrad]
	140348618172688 -> 140348618173024
	140348618172688 [label=TBackward0]
	140348618171584 -> 140348618172688
	140348618119024 [label="neighGen.encoder.layers.1.lin_l.weight
 (128, 64)" fillcolor=lightblue]
	140348618119024 -> 140348618171584
	140348618171584 [label=AccumulateGrad]
	140348618170864 -> 140348618173504
	140348618170864 [label=MmBackward0]
	140348618173840 -> 140348618170864
	140348618171872 -> 140348618170864
	140348618171872 [label=TBackward0]
	140348618172544 -> 140348618171872
	140348618120224 [label="neighGen.encoder.layers.1.lin_r.weight
 (128, 64)" fillcolor=lightblue]
	140348618120224 -> 140348618172544
	140348618172544 [label=AccumulateGrad]
	140348618174272 -> 140348618173648
	140348618174272 [label=TBackward0]
	140348618174032 -> 140348618174272
	140348618120304 [label="neighGen.dGen.reg.weight
 (1, 128)" fillcolor=lightblue]
	140348618120304 -> 140348618174032
	140348618174032 [label=AccumulateGrad]
	140348618198080 -> 140348614687296
}
